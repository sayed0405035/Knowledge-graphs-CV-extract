{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a62ca23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-14T15:57:33.690183Z",
     "iopub.status.busy": "2022-09-14T15:57:33.688991Z",
     "iopub.status.idle": "2022-09-14T15:57:33.716225Z",
     "shell.execute_reply": "2022-09-14T15:57:33.715182Z"
    },
    "papermill": {
     "duration": 0.042638,
     "end_time": "2022-09-14T15:57:33.719040",
     "exception": false,
     "start_time": "2022-09-14T15:57:33.676402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/traindatacv/train_data.pkl\n",
      "/kaggle/input/cvalltogether/CV_brief.pdf\n",
      "/kaggle/input/cvalltogether/drmperfect_cv-april-2020.pdf\n",
      "/kaggle/input/cvalltogether/vita_external.pdf\n",
      "/kaggle/input/cvalltogether/Klar CV.pdf\n",
      "/kaggle/input/cvalltogether/blee_cv_2016.pdf\n",
      "/kaggle/input/cvalltogether/Canales_Robert_CV.pdf\n",
      "/kaggle/input/cvalltogether/MOORE-MONROY2015_0.pdf\n",
      "/kaggle/input/cvalltogether/CURRICULUM-VITAE_DHG_012519.pdf\n",
      "/kaggle/input/cvalltogether/RobertsonCV0818-2.pdf\n",
      "/kaggle/input/cvalltogether/hameroff2016cv_0.pdf\n",
      "/kaggle/input/cvalltogether/Alison-M-Meadow-cv.pdf\n",
      "/kaggle/input/cvalltogether/agaspar_cv.pdf\n",
      "/kaggle/input/cvalltogether/JO - 2171.pdf\n",
      "/kaggle/input/cvalltogether/Hoit CV (4-11-16).pdf\n",
      "/kaggle/input/cvalltogether/Liverman Selected CV May 2018.pdf\n",
      "/kaggle/input/cvalltogether/LBarraza CV 2020.pdf\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5692b029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:57:33.738293Z",
     "iopub.status.busy": "2022-09-14T15:57:33.737832Z",
     "iopub.status.idle": "2022-09-14T15:58:16.852184Z",
     "shell.execute_reply": "2022-09-14T15:58:16.850382Z"
    },
    "papermill": {
     "duration": 43.127173,
     "end_time": "2022-09-14T15:58:16.855048",
     "exception": false,
     "start_time": "2022-09-14T15:57:33.727875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\r\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fe57d55b810>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f3/76/74f4d596f847ae7d6b57749cd245393e357472c12a3dfc1acd738773b7d7/PyPDF2-2.10.8-py3-none-any.whl\u001b[0m\u001b[33m\r\n",
      "\u001b[0m  Downloading PyPDF2-2.10.8-py3-none-any.whl (217 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.7/217.7 kB\u001b[0m \u001b[31m952.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.7/site-packages (from PyPDF2) (4.3.0)\r\n",
      "Installing collected packages: PyPDF2\r\n",
      "Successfully installed PyPDF2-2.10.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.7.4-py3-none-any.whl (40 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m644.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (9.1.1)\r\n",
      "Collecting pdfminer.six==20220524\r\n",
      "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Wand>=0.6.7 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (0.6.10)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20220524->pdfplumber) (2.1.0)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20220524->pdfplumber) (37.0.2)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (2.21)\r\n",
      "Installing collected packages: pdfminer.six, pdfplumber\r\n",
      "Successfully installed pdfminer.six-20220524 pdfplumber-0.7.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mcomplete\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install pdfplumber\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9486e922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:16.878990Z",
     "iopub.status.busy": "2022-09-14T15:58:16.878468Z",
     "iopub.status.idle": "2022-09-14T15:58:30.487788Z",
     "shell.execute_reply": "2022-09-14T15:58:30.486607Z"
    },
    "papermill": {
     "duration": 13.62542,
     "end_time": "2022-09-14T15:58:30.491240",
     "exception": false,
     "start_time": "2022-09-14T15:58:16.865820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#all import\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#to store data\n",
    "allCVData=[]\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c51bb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.522365Z",
     "iopub.status.busy": "2022-09-14T15:58:30.520410Z",
     "iopub.status.idle": "2022-09-14T15:58:30.535003Z",
     "shell.execute_reply": "2022-09-14T15:58:30.533060Z"
    },
    "papermill": {
     "duration": 0.033056,
     "end_time": "2022-09-14T15:58:30.538199",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.505143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# to do : do partial match , i.e. word+etc\n",
    "\n",
    "reference=[ \n",
    "            #node1\n",
    "            \"articles\",\"article\",\"books\",\"book\",\"chapters\",\"chapter\",\"citations\",\"citation\",\"editorials\",\"editorial\",\"journals\",\"journal\",\n",
    "               \"seminars\",\"seminar\",\"scholarly\",\n",
    "            #node2\n",
    "            \"awards\",\"award\",\"proposals\",\"proposal\",\"grants\",\"grant\",\"honors\",\"honor\",\"scholarships\",\"scholarship\",\"sponsored\",\n",
    "            #node3\n",
    "            \"appointments\",\"appointment\",\"experiences\",\"experience\",\"services\",\"service\",\"employments\",\"employment\",\"practices\",\"practice\",\n",
    "                \"professionals\",\"professional\",\n",
    "            #node4\n",
    "            \"affiliations\",\"affiliation\",\"memberships\",\"membership\",\"committees\",\"committee\",\n",
    "            #node5\n",
    "            \"contacts\",\"contact\",\"introductions\",\"introduction\",\n",
    "            #node6\n",
    "            \"publications\",\"publication\",\"conferences\",\"conference\",\"presentations\",\"presentation\",\"newsletters\",\"newsletter\",\"reports\",\"report\",\n",
    "            #node7\n",
    "            \"educations\",\"education\",\"certificates\",\"certificate\",\"certifications\",\"certification\",\n",
    "            #node8\n",
    "            \"researches\",\"research\",\n",
    "            #node9\n",
    "            \"teaching\",\"outreaches\",\"outreach\"\n",
    "          ]\n",
    "\n",
    "#allCVData.append((\"fileName\",\"personName\",\"sectionName\",\"head\",\"relation/lebel\",\"tail\"))\n",
    "\n",
    "#print(allCVData)\n",
    "#print(reference,len(reference))\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f88cdc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.565313Z",
     "iopub.status.busy": "2022-09-14T15:58:30.564023Z",
     "iopub.status.idle": "2022-09-14T15:58:30.577405Z",
     "shell.execute_reply": "2022-09-14T15:58:30.575626Z"
    },
    "papermill": {
     "duration": 0.030382,
     "end_time": "2022-09-14T15:58:30.581134",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.550752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def dfPartByPart(textList):\n",
    "    collectedText=[]\n",
    "    collectedByDictionary=dict()\n",
    "    collectedParts=dict()\n",
    "    keyDictionary=\"introductions\"\n",
    "    temp=[]\n",
    "\n",
    "    \n",
    "\n",
    "    for smallPartText in textList:\n",
    "        if smallPartText==None:\n",
    "            continue\n",
    "   \n",
    "        for partOfSmallPartText in smallPartText.split():\n",
    "            if len(partOfSmallPartText)<2:\n",
    "                continue\n",
    "            result = list(filter(lambda x: x==partOfSmallPartText.lower(), reference))\n",
    "            \n",
    "            if result:\n",
    "                #print(result,keyDictionary,temp,smallPartText)\n",
    "                #print(result,smallPartText)\n",
    "                collectedText.append(temp)\n",
    "                if keyDictionary in collectedByDictionary:\n",
    "                    collectedByDictionary[keyDictionary]=collectedByDictionary[keyDictionary]+temp\n",
    "                else:\n",
    "                    collectedByDictionary[keyDictionary]=temp\n",
    "                keyDictionary=result[0]\n",
    "                #print(result,keyDictionary)\n",
    "                temp=[]\n",
    "                break\n",
    "        \n",
    "        temp.append(smallPartText)\n",
    "    \n",
    "    \n",
    "    collectedText.append(temp)\n",
    "    if keyDictionary in collectedByDictionary:\n",
    "        collectedByDictionary[keyDictionary]=collectedByDictionary[keyDictionary]+temp\n",
    "    else:\n",
    "        collectedByDictionary[keyDictionary]=temp\n",
    "\n",
    "    return collectedByDictionary\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94345859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.609808Z",
     "iopub.status.busy": "2022-09-14T15:58:30.609232Z",
     "iopub.status.idle": "2022-09-14T15:58:30.628419Z",
     "shell.execute_reply": "2022-09-14T15:58:30.627139Z"
    },
    "papermill": {
     "duration": 0.035326,
     "end_time": "2022-09-14T15:58:30.631544",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.596218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#import re\n",
    "#from nltk.corpus import stopwords\n",
    "# load pre-trained model\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "# Education Degrees\n",
    "EDUCATIONDEGREE = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S','B.S.','BSC','B.SC','B.SC.','C.A.','B.COM','BCOM',\n",
    "            'M.COM', 'MCOM','M.COM.',\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S','M.S.','MSC','M.SC','M.SC.',\n",
    "            'BTECH', 'B.TECH','B.TECH.', 'M.TECH','M.TECH.', 'MTECH',\n",
    "            'PHD','PH.D', 'PH.D.','MBA','GRADUATE', \n",
    "            'POST-GRADUATE','MASTERS',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "def extract_educationDegree(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = []\n",
    "    \n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        text=text.replace(\",\",\" \")\n",
    "        #print(text)\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            #tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            \n",
    "            if tex.upper() in EDUCATIONDEGREE and tex not in STOPWORDS:\n",
    "                \n",
    "                if tex not in edu:\n",
    "                    edu.append(tex)\n",
    "                                \n",
    "                \n",
    "    return edu\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af410bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.656405Z",
     "iopub.status.busy": "2022-09-14T15:58:30.655880Z",
     "iopub.status.idle": "2022-09-14T15:58:30.668804Z",
     "shell.execute_reply": "2022-09-14T15:58:30.666794Z"
    },
    "papermill": {
     "duration": 0.028707,
     "end_time": "2022-09-14T15:58:30.671451",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.642744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "# Education Degrees\n",
    "Dsignation = [\n",
    "    'assistant professor','associate professor','assistant','affiliate','adjunct faculty',\n",
    "    'associate research professor','associate research scientist','assistant specialist',\n",
    "    'adjunct assistant research scientist',\n",
    "    'instructor',\n",
    "    'manager',\n",
    "    'postdoctoral researcher','program manager','project manager','professor',\n",
    "    'program evaluator','post-doctoral fellow','postdoctoral research fellowship',\n",
    "    'research assistant','research technician',\n",
    "    'senior research associate','staff scientist','seasonal position','specialist',\n",
    "    'teaching assistant','teachers assistant'\n",
    "        ]\n",
    "def extractDsignation(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = []\n",
    "    \n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        text=text.replace(\",\",\" \")\n",
    "        if text.lower() in Dsignation and tex not in STOPWORDS:\n",
    "                \n",
    "            if text not in edu:\n",
    "                edu.append(text)\n",
    "        #print(text)\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            #tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            \n",
    "            if tex.lower() in Dsignation and tex not in STOPWORDS:\n",
    "                \n",
    "                if tex not in edu:\n",
    "                    edu.append(tex)\n",
    "                                \n",
    "                \n",
    "    return edu\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baafa03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.697159Z",
     "iopub.status.busy": "2022-09-14T15:58:30.696392Z",
     "iopub.status.idle": "2022-09-14T15:58:30.704553Z",
     "shell.execute_reply": "2022-09-14T15:58:30.703040Z"
    },
    "papermill": {
     "duration": 0.023184,
     "end_time": "2022-09-14T15:58:30.706920",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.683736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def newGetDate(dataMaybeDate):\n",
    "    match = re.search('\\d{4}',dataMaybeDate)\n",
    "    match001=re.search('\\d{2}/\\d{2}/\\d{4}',dataMaybeDate)\n",
    "    \n",
    "    #if match001 is not None and int(match001)>1900 and int(match001)<2023:\n",
    "    if match001:\n",
    "        # Then it found a match!\n",
    "        #print(type(match001))\n",
    "        return match001.group(0)\n",
    "    \n",
    "    if match is not None :\n",
    "        # Then it found a match!\n",
    "        if int(match.group(0))>1900 and int(match.group(0))<2023:\n",
    "            return match.group(0)\n",
    "        return None\n",
    "    return None\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a6e102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.731883Z",
     "iopub.status.busy": "2022-09-14T15:58:30.731103Z",
     "iopub.status.idle": "2022-09-14T15:58:30.739779Z",
     "shell.execute_reply": "2022-09-14T15:58:30.738579Z"
    },
    "papermill": {
     "duration": 0.024533,
     "end_time": "2022-09-14T15:58:30.742588",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.718055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def findInstitute(tika_text):\n",
    "    sub_patterns = ['[A-Z][a-z]* University',\n",
    "                    '[A-Z][a-z]* Educational Institute',\n",
    "                '[A-Z][a-z]* College',\n",
    "                'University of [A-Z][a-z]*',\n",
    "                'The University of [A-Z][a-z]*',\n",
    "                    'TheUniversityof[A-Z][a-z]*',\n",
    "                'Ecole [A-Z][a-z]*',\n",
    "                   '[A-Z][a-z]*University',\n",
    "                    '[A-Z][a-z]*EducationalInstitute',\n",
    "                '[A-Z][a-z]*College',\n",
    "                'Universityof[A-Z][a-z]*',\n",
    "                'Ecole[A-Z][a-z]*',\n",
    "                    'The [A-Z][a-z]* Academy of [A-Z][a-z]*',\n",
    "                    'the [A-Z][a-z]* academy of [A-Z][a-z]*'\n",
    "                   ]\n",
    "    pattern = '({})'.format('|'.join(sub_patterns))\n",
    "    matches = re.findall(pattern, tika_text)\n",
    "\n",
    "    return matches\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25b8766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.767164Z",
     "iopub.status.busy": "2022-09-14T15:58:30.766681Z",
     "iopub.status.idle": "2022-09-14T15:58:30.774600Z",
     "shell.execute_reply": "2022-09-14T15:58:30.773101Z"
    },
    "papermill": {
     "duration": 0.023015,
     "end_time": "2022-09-14T15:58:30.776873",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.753858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def removePrefix(dataSent):\n",
    "    resDataIndex=0\n",
    "    for i in dataSent:\n",
    "        #if i.isalpha():\n",
    "        if i.isalnum():\n",
    "            #print(i,resDataIndex)\n",
    "            break\n",
    "        resDataIndex+=1\n",
    "    dataSent=dataSent[resDataIndex:]\n",
    "    return dataSent\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947406b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:30.802601Z",
     "iopub.status.busy": "2022-09-14T15:58:30.801296Z",
     "iopub.status.idle": "2022-09-14T15:58:31.756256Z",
     "shell.execute_reply": "2022-09-14T15:58:31.754638Z"
    },
    "papermill": {
     "duration": 0.970602,
     "end_time": "2022-09-14T15:58:31.759210",
     "exception": false,
     "start_time": "2022-09-14T15:58:30.788608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern], on_match = None)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    res=[]\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        temp=span.text\n",
    "        if len(temp)>0:\n",
    "            lastoption=temp\n",
    "        if (\"vitae\" in temp.lower() or \"sciences\" in temp.lower() \n",
    "            or \"engineering\" in temp.lower() or \"biographical\" in temp.lower() \n",
    "            or \"no\" in temp.lower() or \"title\" in temp.lower() ):\n",
    "            continue\n",
    "        res.append(temp)\n",
    "    \n",
    "    return res\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719cf980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:31.784500Z",
     "iopub.status.busy": "2022-09-14T15:58:31.783634Z",
     "iopub.status.idle": "2022-09-14T15:58:31.790280Z",
     "shell.execute_reply": "2022-09-14T15:58:31.788918Z"
    },
    "papermill": {
     "duration": 0.022915,
     "end_time": "2022-09-14T15:58:31.793312",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.770397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#E-MAIL\n",
    "#import re\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dfb2c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:31.818709Z",
     "iopub.status.busy": "2022-09-14T15:58:31.818177Z",
     "iopub.status.idle": "2022-09-14T15:58:31.833355Z",
     "shell.execute_reply": "2022-09-14T15:58:31.832469Z"
    },
    "papermill": {
     "duration": 0.030825,
     "end_time": "2022-09-14T15:58:31.836136",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.805311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleEducations(listOfEducations):\n",
    "    eduData=[]\n",
    "    pieceEduData=dict()\n",
    "    rest=\"\"\n",
    "    for eduPart in listOfEducations:\n",
    "        #print(type(eduPart))\n",
    "        \n",
    "        res=extract_educationDegree(eduPart)\n",
    "        if res:\n",
    "            if len(pieceEduData)!=0:\n",
    "                if len(rest)>1:\n",
    "                    pieceEduData[\"total details\"]=rest\n",
    "                    rest=\"\"\n",
    "                eduData.append(pieceEduData)\n",
    "                pieceEduData=dict()\n",
    "            pieceEduData[\"degree\"]=res[0]\n",
    "            index001=eduPart.find(res[0])\n",
    "            eduPart=eduPart[:index001]+eduPart[index001+len(res[0]):]\n",
    "            #print(res,index001,eduPart)\n",
    "            #print(res,eduPart)\n",
    "        #datePart=getDate(eduPart)\n",
    "        newDatePart=newGetDate(eduPart)\n",
    "        \n",
    "        #if datePart:\n",
    "        #    print(\"date:\",datePart)\n",
    "        if newDatePart:\n",
    "            pieceEduData[\"date\"]=newDatePart\n",
    "            index002=eduPart.find(newDatePart)\n",
    "            eduPart=eduPart[:index002]+eduPart[index002+len(newDatePart):]\n",
    "            #print(index002,\"date:\",newDatePart,eduPart)\n",
    "        \n",
    "        #test001=eduPart.split(\":\")\n",
    "        #for itest in test001:\n",
    "        if \"advisor\" in eduPart.lower():\n",
    "            \n",
    "            index003=eduPart.lower().find(\"advisor\")\n",
    "            advis001=eduPart[index003+len(\"advisor\"):]\n",
    "            advis001=removePrefix(advis001)\n",
    "            pieceEduData[\"advisor\"]=advis001\n",
    "            eduPart=eduPart[:index003]\n",
    "            #print(\"advisor\",advis001,eduPart)\n",
    "        org=findInstitute(eduPart)\n",
    "        if org:\n",
    "            pieceEduData[\"organization\"]=org[0]\n",
    "            index004=eduPart.find(org[0])\n",
    "            eduPart=eduPart[:index004]+eduPart[index004+len(org[0]):]\n",
    "            #print(org,eduPart)\n",
    "        #print(eduPart)\n",
    "        eduPart=removePrefix(eduPart)\n",
    "        \n",
    "        designation=extractDsignation(eduPart)\n",
    "        if designation:\n",
    "            pieceEduData[\"designation\"]=designation[0]\n",
    "            index004=eduPart.find(designation[0])\n",
    "            eduPart=eduPart[:index004]+eduPart[index004+len(designation[0]):]\n",
    "        \n",
    "        if len(eduPart)>1:\n",
    "            rest+=eduPart\n",
    "        #print(eduPart)\n",
    "        \n",
    "        \n",
    "    if len(pieceEduData)!=0:\n",
    "        if len(rest)>1:\n",
    "            pieceEduData[\"total details\"]=rest\n",
    "                    #rest=\"\"\n",
    "        eduData.append(pieceEduData)\n",
    "    return eduData\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "815b6778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:31.861219Z",
     "iopub.status.busy": "2022-09-14T15:58:31.859937Z",
     "iopub.status.idle": "2022-09-14T15:58:31.870246Z",
     "shell.execute_reply": "2022-09-14T15:58:31.869108Z"
    },
    "papermill": {
     "duration": 0.026123,
     "end_time": "2022-09-14T15:58:31.873472",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.847349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleContacts(sentDataContacts):\n",
    "    \n",
    "    contactDetails=dict()\n",
    "    \n",
    "    #otherDetails=[]\n",
    "    \n",
    "    #print(sentDataContacts)\n",
    "    textContacts001=\",\".join(sentDataContacts)\n",
    "    nameContacts001=extract_name(textContacts001)\n",
    "    if nameContacts001:\n",
    "        contactDetails[\"name\"]=nameContacts001[0]\n",
    "    else:\n",
    "        contactDetails[\"name\"]=sentDataContacts[0]\n",
    "    \n",
    "    #flagContact=False\n",
    "    othersRests=[]\n",
    "    #print(nameContacts001[0],nameContacts001)\n",
    "    for contacts00 in sentDataContacts:\n",
    "        #dateFound001=newGetDate(contacts00)\n",
    "        #print(dateFound001,contacts00)\n",
    "        \n",
    "        \n",
    "        \n",
    "        e_mailContacts=get_email_addresses(contacts00)\n",
    "        if e_mailContacts:\n",
    "            #print(e_mailContacts,contacts00)\n",
    "            #contactIndex001=contacts00.lower().find(e_mailContacts)\n",
    "            contactIndex001=contacts00.find(e_mailContacts[0])\n",
    "            \n",
    "            contacts00=contacts00[:contactIndex001]+contacts00[contactIndex001+len(e_mailContacts[0]):]\n",
    "            contacts00=removePrefix(contacts00)\n",
    "            \n",
    "            \n",
    "            contactDetails[\"e-mails\"]=e_mailContacts[0]\n",
    "            #print(e_mailContacts,contacts00)\n",
    "        othersRests.append(contacts00)\n",
    "    contactDetails[\"other_details\"]=\" \".join(othersRests)\n",
    "    return contactDetails\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86bef66c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:31.898160Z",
     "iopub.status.busy": "2022-09-14T15:58:31.897665Z",
     "iopub.status.idle": "2022-09-14T15:58:31.924299Z",
     "shell.execute_reply": "2022-09-14T15:58:31.923247Z"
    },
    "papermill": {
     "duration": 0.042321,
     "end_time": "2022-09-14T15:58:31.927095",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.884774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleRestPart(experiencePart):\n",
    "    allExperience=[]\n",
    "    detailsExperience=\"\"\n",
    "    tempExperience=dict()\n",
    "    flag=False # need for different in the 1st\n",
    "    for partExperience in experiencePart:\n",
    "        dateFound=newGetDate(partExperience)\n",
    "        if dateFound:\n",
    "            \n",
    "            \n",
    "            if not flag:\n",
    "                flag=True\n",
    "                tempExperience['year']=dateFound\n",
    "                partExperienceIndex001=partExperience.find(dateFound)\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dateFound):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                detailsExperience+=\" \"+partExperience\n",
    "                \n",
    "                organizationExperience=findInstitute(partExperience)\n",
    "        \n",
    "                if organizationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(organizationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['organization']=organizationExperience[0]\n",
    "                \n",
    "                dsignationExperience=extractDsignation(partExperience)\n",
    "                if dsignationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['dsignation']=dsignationExperience[0]\n",
    "                \n",
    "                continue\n",
    "            else:\n",
    "                tempExperience['details']=detailsExperience\n",
    "                detailsExperience=\"\"\n",
    "                allExperience.append(tempExperience)\n",
    "                \n",
    "                tempExperience=dict()\n",
    "                tempExperience['year']=dateFound\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(dateFound)\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dateFound):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                organizationPublications=findInstitute(partExperience)\n",
    "        \n",
    "                if organizationPublications:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(organizationPublications[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationPublications[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['organization']=organizationPublications[0]\n",
    "                dsignationExperience=extractDsignation(partExperience)\n",
    "                if dsignationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['dsignation']=dsignationExperience[0]\n",
    "                \n",
    "                detailsExperience+=\" \"+partExperience\n",
    "        else:\n",
    "            partExperience=removePrefix(partExperience)\n",
    "            \n",
    "            organizationPublications=findInstitute(partExperience)\n",
    "        \n",
    "            if organizationPublications:\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(organizationPublications[0])\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationPublications[0]):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                tempExperience['organization']=organizationPublications[0]\n",
    "            \n",
    "            \n",
    "            dsignationExperience=extractDsignation(partExperience)\n",
    "            if dsignationExperience:\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                tempExperience['dsignation']=dsignationExperience[0]\n",
    "            \n",
    "            detailsExperience+=\" \"+partExperience\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # extracting name gives a lot of false result, \n",
    "        #as it contains name of things other than person name\n",
    "        \n",
    "        #namepartExperience=extract_name(partExperience)\n",
    "        #if namepartExperience:\n",
    "            #print(namepartExperience,type(namepartExperience))\n",
    "    \n",
    "        \n",
    "        \n",
    "    if tempExperience:\n",
    "        tempExperience['details']=detailsExperience\n",
    "        allExperience.append(tempExperience)\n",
    "            \n",
    "    return allExperience\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a36b1",
   "metadata": {
    "papermill": {
     "duration": 0.011142,
     "end_time": "2022-09-14T15:58:31.949941",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.938799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5881ece1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:31.975974Z",
     "iopub.status.busy": "2022-09-14T15:58:31.974923Z",
     "iopub.status.idle": "2022-09-14T15:58:33.625687Z",
     "shell.execute_reply": "2022-09-14T15:58:33.624600Z"
    },
    "papermill": {
     "duration": 1.666576,
     "end_time": "2022-09-14T15:58:33.628050",
     "exception": false,
     "start_time": "2022-09-14T15:58:31.961474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#import pdfplumber\n",
    "#import pandas as pd\n",
    "#import os\n",
    "# Importing required modules\n",
    "import PyPDF2\n",
    "\n",
    "def extract_pdf(pdf_path):\n",
    "    linesOfFile = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for pdf_page in pdf.pages:\n",
    "            single_page_text = pdf_page.extract_text()\n",
    "            for line in single_page_text.split('\\n'):\n",
    "                linesOfFile.append(line)\n",
    "                #print(linesOfFile)\n",
    "    return linesOfFile\n",
    "\n",
    "\n",
    "folder_with_pdfs = '../input/cvalltogether'\n",
    "linesOfFiles = []\n",
    "\n",
    "#specially for testing ner\n",
    "dataForTest=dict()\n",
    "\n",
    "listOfPdfFiles=[]\n",
    "\n",
    "for pdf_file in os.listdir(folder_with_pdfs):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        \n",
    "        #print(pdf_file)\n",
    "        listOfPdfFiles.append(pdf_file)\n",
    "        pdf_file_path = os.path.join(folder_with_pdfs, pdf_file)\n",
    "        \n",
    "        pdfFileObj = open(pdf_file_path,'rb')\n",
    "        \n",
    "        #print(pdfFileObj)\n",
    "        \n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# Getting number of pages in pdf file\n",
    "        pages = pdfReader.numPages\n",
    "\n",
    "        totalText=[]\n",
    "\n",
    "# Loop for reading all the Pages\n",
    "        for i in range(pages):\n",
    "\n",
    "        # Creating a page object\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "\n",
    "        # Printing Page Number\n",
    "        #print(\"Page No: \",i)\n",
    "\n",
    "        # Extracting text from page\n",
    "        # And splitting it into chunks of lines\n",
    "            text = pageObj.extractText().split(\"\\n\")\n",
    "        \n",
    "            totalText+=text\n",
    "        \n",
    "        \n",
    "# closing the pdf file object\n",
    "        pdfFileObj.close()\n",
    "        #print(totalText)\n",
    "        \n",
    "        #sectionDictionary=dfPartByPart(totalText)\n",
    "        #newSectionDictionary=reDistribute(sectionDictionary)\n",
    "        \n",
    "        \n",
    "        newSectionDictionary=dfPartByPart(totalText)\n",
    "        \n",
    "        #specially for testing ner\n",
    "        dataForTest=newSectionDictionary\n",
    "        \n",
    "        tempData00=[]\n",
    "        \n",
    "        contactPart001,educationsPart001=dict(),dict()\n",
    "        \n",
    "        personName00=\"\"\n",
    "        personNameLast=\"\"\n",
    "        if 'contacts' in newSectionDictionary:\n",
    "            text00=newSectionDictionary['contacts']\n",
    "            personName00=extract_name(\",\".join(text00))\n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        \n",
    "        if len(personName00)==0:\n",
    "            if 'introductions' in newSectionDictionary:\n",
    "                text00=newSectionDictionary['introductions']\n",
    "                personName00=extract_name(\",\".join(text00))\n",
    "            \n",
    "                \n",
    "            if len(personName00)==0:\n",
    "                personNameLast=\"no name\"\n",
    "            else:\n",
    "             \n",
    "            \n",
    "                personNameLast=personName00[0]\n",
    "        else:\n",
    "             \n",
    "            \n",
    "            personNameLast=personName00[0]\n",
    "        #print(i,personNameLast)\n",
    "\n",
    "        \n",
    "        \n",
    "        for partSections in newSectionDictionary:\n",
    "            tempData00=[personNameLast]\n",
    "            if (partSections=='contacts' or partSections=='introductions') and newSectionDictionary[partSections]:\n",
    "                tempData00.append(partSections+\" details\")\n",
    "                tempData00.append(partSections)\n",
    "                contactPart001=handleContacts(newSectionDictionary[partSections])\n",
    "                for parts in contactPart001:\n",
    "                    tempData00.append(parts)\n",
    "                    tempData00.append(contactPart001[parts])\n",
    "                \n",
    "                #print(tempData00)\n",
    "                    if len(tempData00)>1:\n",
    "                        allCVData.append(tempData00)\n",
    "                \n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "            \n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "            \n",
    "            #print(contactPart001) \n",
    "            #done for now\n",
    "            elif (partSections=='educations' or partSections=='certificates' or partSections=='certifications') and newSectionDictionary[partSections]:\n",
    "                tempData00.append(\"qualifications details\")\n",
    "                tempData00.append(partSections)\n",
    "                educationsPart001=handleEducations(newSectionDictionary[partSections])\n",
    "                for parts001 in educationsPart001:\n",
    "                    tempData00.append(\"degree earned\")\n",
    "                    if 'degree' in parts001:\n",
    "                        tempData00.append(parts001['degree'])\n",
    "                    else:\n",
    "                        tempData00.append(\"Degree\")\n",
    "                \n",
    "                    for smallparts002 in parts001:\n",
    "                        tempData00.append(smallparts002)\n",
    "                        tempData00.append(parts001[smallparts002])\n",
    "                    \n",
    "                    #print(tempData00)\n",
    "                        if len(tempData00)>1:\n",
    "                            allCVData.append(tempData00.copy())\n",
    "                    \n",
    "                        tempData00.pop()\n",
    "                        tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "            else:\n",
    "                tempEdge=partSections +\" experience\"\n",
    "        \n",
    "                tempData00.append(tempEdge)\n",
    "                tempData00.append(partSections)\n",
    "                restPart001=handleRestPart(newSectionDictionary[partSections])\n",
    "        \n",
    "                count=1\n",
    "                for parts001 in restPart001:\n",
    "                    newTempEdge=partSections +\" details\"\n",
    "                    tempData00.append(newTempEdge)\n",
    "                    tempData00.append(partSections+\" no.\"+str(count))\n",
    "            \n",
    "            \n",
    "                \n",
    "                    for smallparts002 in parts001:\n",
    "                        tempData00.append(smallparts002)\n",
    "                        tempData00.append(parts001[smallparts002])\n",
    "                    \n",
    "                    #print(tempData00)\n",
    "                        if len(tempData00)>1:\n",
    "                            allCVData.append(tempData00.copy())\n",
    "                    \n",
    "                        tempData00.pop()\n",
    "                        tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    count+=1\n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "#print(\"complete\")\n",
    "        #break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "        \n",
    "#df = pd.DataFrame(linesOfFiles)\n",
    "#df.to_csv('test.csv')\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "233ab8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:33.653655Z",
     "iopub.status.busy": "2022-09-14T15:58:33.652493Z",
     "iopub.status.idle": "2022-09-14T15:58:33.660237Z",
     "shell.execute_reply": "2022-09-14T15:58:33.658642Z"
    },
    "papermill": {
     "duration": 0.02314,
     "end_time": "2022-09-14T15:58:33.662699",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.639559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'year', '1995'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'organization', 'York University'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'details', ' Education Ph.D. Courant Institute, New York University, New York, NY.'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'year', '1990'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'organization', 'Stanford University'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'details', ' B.S. , Stanford, CA.'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.3', 'year', '1986'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.3', 'details', ' Diploma Florida High School, Tallahassee, FL.'], ['Sunder Sethuraman', 'professional experience', 'professional', 'professional details', 'professional no.1', 'year', '2011'], ['Sunder Sethuraman', 'professional experience', 'professional', 'professional details', 'professional no.1', 'dsignation', 'Professor']]\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "totalAllCVData=[]\n",
    "for check in allCVData:\n",
    "    if len(check)>1:\n",
    "        totalAllCVData.append(check)\n",
    "print(totalAllCVData[:10])\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f24076b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:33.688700Z",
     "iopub.status.busy": "2022-09-14T15:58:33.687408Z",
     "iopub.status.idle": "2022-09-14T15:58:33.697144Z",
     "shell.execute_reply": "2022-09-14T15:58:33.695717Z"
    },
    "papermill": {
     "duration": 0.026233,
     "end_time": "2022-09-14T15:58:33.700609",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.674376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#write.csv(allCVData, \"mycsv.csv\")\n",
    "\n",
    "#header = ['head','relation','tail']\n",
    "\n",
    "with open('CVpdftotalAllCVData001.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header , already has a header\n",
    "    #writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(totalAllCVData)\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0337b",
   "metadata": {
    "papermill": {
     "duration": 0.012081,
     "end_time": "2022-09-14T15:58:33.725156",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.713075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb12d0b",
   "metadata": {
    "papermill": {
     "duration": 0.011473,
     "end_time": "2022-09-14T15:58:33.748676",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.737203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875d0e6",
   "metadata": {
    "papermill": {
     "duration": 0.011283,
     "end_time": "2022-09-14T15:58:33.771637",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.760354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429851c1",
   "metadata": {
    "papermill": {
     "duration": 0.011296,
     "end_time": "2022-09-14T15:58:33.794635",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.783339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2dbcb",
   "metadata": {
    "papermill": {
     "duration": 0.011198,
     "end_time": "2022-09-14T15:58:33.817438",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.806240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c0b9d",
   "metadata": {
    "papermill": {
     "duration": 0.011066,
     "end_time": "2022-09-14T15:58:33.840219",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.829153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d666d",
   "metadata": {
    "papermill": {
     "duration": 0.01178,
     "end_time": "2022-09-14T15:58:33.863613",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.851833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fe6e974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:33.889850Z",
     "iopub.status.busy": "2022-09-14T15:58:33.889008Z",
     "iopub.status.idle": "2022-09-14T15:58:34.673373Z",
     "shell.execute_reply": "2022-09-14T15:58:34.671441Z"
    },
    "papermill": {
     "duration": 0.801114,
     "end_time": "2022-09-14T15:58:34.676509",
     "exception": false,
     "start_time": "2022-09-14T15:58:33.875395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en')\n",
    "def nerCV(personName,sectionName,sentDataFoeTest):\n",
    "    allDataner=[]\n",
    "    metadata001=[personName,sectionName+\" with\",sectionName]\n",
    "    for train_dataText in sentDataFoeTest :\n",
    "        \n",
    "    \n",
    "    #train_dataText=\" \".join(sentDataFoeTest)\n",
    "    #doc = loaded_nlp_model(train_dataText)\n",
    "    \n",
    "        doc = nlp(train_dataText)\n",
    "        #print(doc)\n",
    "        \n",
    "        dldata=dict()\n",
    "    #print(doc)\n",
    "        #print(\"*************\")\n",
    "        for ent in doc.ents:\n",
    "        #print(\"as\")\n",
    "            if ent.label_.upper() in dldata:\n",
    "                dldata[ent.label_.upper()].append(ent.text)\n",
    "            else:\n",
    "                dldata[ent.label_.upper()]=[ent.text]\n",
    "            #print(ent.label_.upper())\n",
    "            #print(f'{ent.label_.upper():{30}}- {ent.text}')\n",
    "    #break\n",
    "        if dldata:\n",
    "            #print(dldata)\n",
    "            #$allDataner.append(dldata)\n",
    "            for dl in dldata:\n",
    "                metadata=metadata001.copy()\n",
    "                if dl=='DATE':\n",
    "                    for d in dldata[dl]:\n",
    "                        metadata.append(sectionName+ \" in \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        #print(metadata)\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='ORG':\n",
    "                    for d in dldata[dl]:\n",
    "                        metadata.append(sectionName+ \" with \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='PERSON':\n",
    "                    for d in dldata[dl]:\n",
    "                        metadata.append(sectionName+ \" associated with \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='GPE':\n",
    "                    for d in dldata[dl]:\n",
    "                        metadata.append(sectionName+ \" from \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                else:\n",
    "                    for d in dldata[dl]:\n",
    "                        metadata.append(sectionName+ \" in \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "    #print(allDataner)\n",
    "    return allDataner\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d11b47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:58:34.703474Z",
     "iopub.status.busy": "2022-09-14T15:58:34.702983Z",
     "iopub.status.idle": "2022-09-14T16:00:22.367388Z",
     "shell.execute_reply": "2022-09-14T16:00:22.366028Z"
    },
    "papermill": {
     "duration": 107.692194,
     "end_time": "2022-09-14T16:00:22.381140",
     "exception": false,
     "start_time": "2022-09-14T15:58:34.688946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "folder_with_pdfs = '../input/cvalltogether'\n",
    "linesOfFiles = []\n",
    "\n",
    "#specially for testing ner\n",
    "dataForTest=dict()\n",
    "\n",
    "listOfPdfFiles=[]\n",
    "\n",
    "alnerCVData=[]\n",
    "\n",
    "for pdf_file in os.listdir(folder_with_pdfs):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        \n",
    "        #print(pdf_file)\n",
    "        listOfPdfFiles.append(pdf_file)\n",
    "        pdf_file_path = os.path.join(folder_with_pdfs, pdf_file)\n",
    "        \n",
    "        pdfFileObj = open(pdf_file_path,'rb')\n",
    "        \n",
    "        #print(pdfFileObj)\n",
    "        \n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# Getting number of pages in pdf file\n",
    "        pages = pdfReader.numPages\n",
    "\n",
    "        totalText=[]\n",
    "\n",
    "# Loop for reading all the Pages\n",
    "        for i in range(pages):\n",
    "\n",
    "        # Creating a page object\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "\n",
    "        # Printing Page Number\n",
    "        #print(\"Page No: \",i)\n",
    "\n",
    "        # Extracting text from page\n",
    "        # And splitting it into chunks of lines\n",
    "            text = pageObj.extractText().split(\"\\n\")\n",
    "        \n",
    "            totalText+=text\n",
    "        \n",
    "        \n",
    "# closing the pdf file object\n",
    "        pdfFileObj.close()\n",
    "        #print(totalText)\n",
    "        \n",
    "        #sectionDictionary=dfPartByPart(totalText)\n",
    "        #newSectionDictionary=reDistribute(sectionDictionary)\n",
    "        \n",
    "        \n",
    "        newSectionDictionary=dfPartByPart(totalText)\n",
    "        \n",
    "        personName00=\"\"\n",
    "        personNameLast=\"\"\n",
    "        if 'contacts' in newSectionDictionary:\n",
    "            text00=newSectionDictionary['contacts']\n",
    "            personName00=extract_name(\",\".join(text00))\n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        \n",
    "        if len(personName00)==0:\n",
    "            if 'introductions' in newSectionDictionary:\n",
    "                text00=newSectionDictionary['introductions']\n",
    "                personName00=extract_name(\",\".join(text00))\n",
    "            \n",
    "                \n",
    "            if len(personName00)==0:\n",
    "                personNameLast=\"no name\"\n",
    "            else:\n",
    "             \n",
    "            \n",
    "                personNameLast=personName00[0]\n",
    "        else:\n",
    "             \n",
    "            \n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        #specially for testing ner\n",
    "        dataForTest=newSectionDictionary\n",
    "        for sectionDataSentTest in dataForTest:\n",
    "            if len(dataForTest[sectionDataSentTest])>1:\n",
    "                nerret=nerCV(personNameLast,sectionDataSentTest,dataForTest[sectionDataSentTest][1:])\n",
    "                #print(nerret)\n",
    "                #alnerCVData.extend()\n",
    "                for i in nerret:\n",
    "                    if i not in alnerCVData:\n",
    "                        alnerCVData.append(i)\n",
    "                \n",
    "#dataForTest\n",
    "        #break\n",
    "#alnerCVData=list(set(alnerCVData))\n",
    "#print(alnerCVData)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f4c8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T16:00:22.408586Z",
     "iopub.status.busy": "2022-09-14T16:00:22.407135Z",
     "iopub.status.idle": "2022-09-14T16:00:22.414749Z",
     "shell.execute_reply": "2022-09-14T16:00:22.413468Z"
    },
    "papermill": {
     "duration": 0.023662,
     "end_time": "2022-09-14T16:00:22.417327",
     "exception": false,
     "start_time": "2022-09-14T16:00:22.393665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunder Sethuraman', 'introductions with', 'introductions', 'introductions with ', 'WWW'], ['Sunder Sethuraman', 'education with', 'education', 'education in ', '1995'], ['Sunder Sethuraman', 'education with', 'education', 'education with ', 'Ph.D. Courant Institute'], ['Sunder Sethuraman', 'education with', 'education', 'education with ', 'New York University'], ['Sunder Sethuraman', 'education with', 'education', 'education from ', 'New York'], ['Sunder Sethuraman', 'education with', 'education', 'education from ', 'NY'], ['Sunder Sethuraman', 'education with', 'education', 'education in ', '1990'], ['Sunder Sethuraman', 'education with', 'education', 'education with ', 'B.S. Stanford University'], ['Sunder Sethuraman', 'education with', 'education', 'education with ', 'Stanford'], ['Sunder Sethuraman', 'education with', 'education', 'education with ', 'CA']]\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "print(alnerCVData[:10])\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d3817",
   "metadata": {
    "papermill": {
     "duration": 0.011462,
     "end_time": "2022-09-14T16:00:22.440719",
     "exception": false,
     "start_time": "2022-09-14T16:00:22.429257",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c3bb86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T16:00:22.467439Z",
     "iopub.status.busy": "2022-09-14T16:00:22.466019Z",
     "iopub.status.idle": "2022-09-14T16:00:23.530688Z",
     "shell.execute_reply": "2022-09-14T16:00:23.529208Z"
    },
    "papermill": {
     "duration": 1.081325,
     "end_time": "2022-09-14T16:00:23.533735",
     "exception": false,
     "start_time": "2022-09-14T16:00:22.452410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sectionDataSentTest in dataForTest:\n",
    "    if len(dataForTest[sectionDataSentTest])>1:\n",
    "        nerret=nerCV(personNameLast,sectionDataSentTest,dataForTest[sectionDataSentTest][1:])\n",
    "        \n",
    "        #print(nerret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84d686a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T16:00:23.560511Z",
     "iopub.status.busy": "2022-09-14T16:00:23.559673Z",
     "iopub.status.idle": "2022-09-14T16:00:23.599651Z",
     "shell.execute_reply": "2022-09-14T16:00:23.597642Z"
    },
    "papermill": {
     "duration": 0.056518,
     "end_time": "2022-09-14T16:00:23.602486",
     "exception": false,
     "start_time": "2022-09-14T16:00:23.545968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "with open('nerCVpdftotalAllCVData001.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header , already has a header\n",
    "    #writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(alnerCVData)\n",
    "\n",
    "\n",
    "print(\"complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 183.338703,
   "end_time": "2022-09-14T16:00:27.143068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-14T15:57:23.804365",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
