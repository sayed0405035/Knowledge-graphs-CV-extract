{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9544280",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-16T18:58:29.196239Z",
     "iopub.status.busy": "2022-09-16T18:58:29.195629Z",
     "iopub.status.idle": "2022-09-16T18:58:29.244848Z",
     "shell.execute_reply": "2022-09-16T18:58:29.243672Z"
    },
    "papermill": {
     "duration": 0.063984,
     "end_time": "2022-09-16T18:58:29.248014",
     "exception": false,
     "start_time": "2022-09-16T18:58:29.184030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cvalltogether/CV_brief.pdf\n",
      "/kaggle/input/cvalltogether/drmperfect_cv-april-2020.pdf\n",
      "/kaggle/input/cvalltogether/vita_external.pdf\n",
      "/kaggle/input/cvalltogether/Klar CV.pdf\n",
      "/kaggle/input/cvalltogether/blee_cv_2016.pdf\n",
      "/kaggle/input/cvalltogether/Canales_Robert_CV.pdf\n",
      "/kaggle/input/cvalltogether/MOORE-MONROY2015_0.pdf\n",
      "/kaggle/input/cvalltogether/CURRICULUM-VITAE_DHG_012519.pdf\n",
      "/kaggle/input/cvalltogether/RobertsonCV0818-2.pdf\n",
      "/kaggle/input/cvalltogether/hameroff2016cv_0.pdf\n",
      "/kaggle/input/cvalltogether/Alison-M-Meadow-cv.pdf\n",
      "/kaggle/input/cvalltogether/agaspar_cv.pdf\n",
      "/kaggle/input/cvalltogether/JO - 2171.pdf\n",
      "/kaggle/input/cvalltogether/Hoit CV (4-11-16).pdf\n",
      "/kaggle/input/cvalltogether/Liverman Selected CV May 2018.pdf\n",
      "/kaggle/input/cvalltogether/LBarraza CV 2020.pdf\n",
      "/kaggle/input/traindatacv/train_data.pkl\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab18561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:58:29.266142Z",
     "iopub.status.busy": "2022-09-16T18:58:29.265551Z",
     "iopub.status.idle": "2022-09-16T18:58:56.649779Z",
     "shell.execute_reply": "2022-09-16T18:58:56.648065Z"
    },
    "papermill": {
     "duration": 27.395152,
     "end_time": "2022-09-16T18:58:56.652173",
     "exception": false,
     "start_time": "2022-09-16T18:58:29.257021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\r\n",
      "  Downloading PyPDF2-2.10.8-py3-none-any.whl (217 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.7/217.7 kB\u001b[0m \u001b[31m480.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.7/site-packages (from PyPDF2) (4.3.0)\r\n",
      "Installing collected packages: PyPDF2\r\n",
      "Successfully installed PyPDF2-2.10.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.7.4-py3-none-any.whl (40 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m395.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pdfminer.six==20220524\r\n",
      "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (9.1.1)\r\n",
      "Requirement already satisfied: Wand>=0.6.7 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (0.6.10)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20220524->pdfplumber) (2.1.0)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20220524->pdfplumber) (37.0.2)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (2.21)\r\n",
      "Installing collected packages: pdfminer.six, pdfplumber\r\n",
      "Successfully installed pdfminer.six-20220524 pdfplumber-0.7.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mcomplete\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install pdfplumber\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce6b729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:58:56.675859Z",
     "iopub.status.busy": "2022-09-16T18:58:56.675430Z",
     "iopub.status.idle": "2022-09-16T18:59:07.778389Z",
     "shell.execute_reply": "2022-09-16T18:59:07.777363Z"
    },
    "papermill": {
     "duration": 11.117825,
     "end_time": "2022-09-16T18:59:07.780969",
     "exception": false,
     "start_time": "2022-09-16T18:58:56.663144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#all import\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#to store data\n",
    "allCVData=[]\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a4283c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.804197Z",
     "iopub.status.busy": "2022-09-16T18:59:07.803549Z",
     "iopub.status.idle": "2022-09-16T18:59:07.812083Z",
     "shell.execute_reply": "2022-09-16T18:59:07.811024Z"
    },
    "papermill": {
     "duration": 0.02299,
     "end_time": "2022-09-16T18:59:07.814879",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.791889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# to do : do partial match , i.e. word+etc\n",
    "\n",
    "reference=[ \n",
    "            #node1\n",
    "            \"articles\",\"article\",\"books\",\"book\",\"chapters\",\"chapter\",\"citations\",\"citation\",\"editorials\",\"editorial\",\"journals\",\"journal\",\n",
    "               \"seminars\",\"seminar\",\"scholarly\",\n",
    "            #node2\n",
    "            \"awards\",\"award\",\"proposals\",\"proposal\",\"grants\",\"grant\",\"honors\",\"honor\",\"scholarships\",\"scholarship\",\"sponsored\",\n",
    "            #node3\n",
    "            \"appointments\",\"appointment\",\"experiences\",\"experience\",\"services\",\"service\",\"employments\",\"employment\",\"practices\",\"practice\",\n",
    "                \"professionals\",\"professional\",\n",
    "            #node4\n",
    "            \"affiliations\",\"affiliation\",\"memberships\",\"membership\",\"committees\",\"committee\",\n",
    "            #node5\n",
    "            \"contacts\",\"contact\",\"introductions\",\"introduction\",\n",
    "            #node6\n",
    "            \"publications\",\"publication\",\"conferences\",\"conference\",\"presentations\",\"presentation\",\"newsletters\",\"newsletter\",\"reports\",\"report\",\n",
    "            #node7\n",
    "            \"educations\",\"education\",\"certificates\",\"certificate\",\"certifications\",\"certification\",\n",
    "            #node8\n",
    "            \"researches\",\"research\",\n",
    "            #node9\n",
    "            \"teaching\",\"outreaches\",\"outreach\"\n",
    "          ]\n",
    "\n",
    "#allCVData.append((\"fileName\",\"personName\",\"sectionName\",\"head\",\"relation/lebel\",\"tail\"))\n",
    "\n",
    "#print(allCVData)\n",
    "#print(reference,len(reference))\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90619069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.837343Z",
     "iopub.status.busy": "2022-09-16T18:59:07.836973Z",
     "iopub.status.idle": "2022-09-16T18:59:07.846714Z",
     "shell.execute_reply": "2022-09-16T18:59:07.845735Z"
    },
    "papermill": {
     "duration": 0.023991,
     "end_time": "2022-09-16T18:59:07.849355",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.825364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def dfPartByPart(textList):\n",
    "    collectedText=[]\n",
    "    collectedByDictionary=dict()\n",
    "    collectedParts=dict()\n",
    "    keyDictionary=\"introductions\"\n",
    "    temp=[]\n",
    "\n",
    "    \n",
    "\n",
    "    for smallPartText in textList:\n",
    "        if smallPartText==None:\n",
    "            continue\n",
    "   \n",
    "        for partOfSmallPartText in smallPartText.split():\n",
    "            if len(partOfSmallPartText)<2:\n",
    "                continue\n",
    "            result = list(filter(lambda x: x==partOfSmallPartText.lower(), reference))\n",
    "            \n",
    "            if result:\n",
    "                #print(result,keyDictionary,temp,smallPartText)\n",
    "                #print(result,smallPartText)\n",
    "                collectedText.append(temp)\n",
    "                if keyDictionary in collectedByDictionary:\n",
    "                    collectedByDictionary[keyDictionary]=collectedByDictionary[keyDictionary]+temp\n",
    "                else:\n",
    "                    collectedByDictionary[keyDictionary]=temp\n",
    "                keyDictionary=result[0]\n",
    "                #print(result,keyDictionary)\n",
    "                temp=[]\n",
    "                break\n",
    "        \n",
    "        temp.append(smallPartText)\n",
    "    \n",
    "    \n",
    "    collectedText.append(temp)\n",
    "    if keyDictionary in collectedByDictionary:\n",
    "        collectedByDictionary[keyDictionary]=collectedByDictionary[keyDictionary]+temp\n",
    "    else:\n",
    "        collectedByDictionary[keyDictionary]=temp\n",
    "\n",
    "    return collectedByDictionary\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58b4178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.873111Z",
     "iopub.status.busy": "2022-09-16T18:59:07.872778Z",
     "iopub.status.idle": "2022-09-16T18:59:07.887149Z",
     "shell.execute_reply": "2022-09-16T18:59:07.886071Z"
    },
    "papermill": {
     "duration": 0.028169,
     "end_time": "2022-09-16T18:59:07.889098",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.860929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#import re\n",
    "#from nltk.corpus import stopwords\n",
    "# load pre-trained model\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "# Education Degrees\n",
    "EDUCATIONDEGREE = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S','B.S.','BSC','B.SC','B.SC.','C.A.','B.COM','BCOM',\n",
    "            'M.COM', 'MCOM','M.COM.',\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S','M.S.','MSC','M.SC','M.SC.',\n",
    "            'BTECH', 'B.TECH','B.TECH.', 'M.TECH','M.TECH.', 'MTECH',\n",
    "            'PHD','PH.D', 'PH.D.','MBA','GRADUATE', \n",
    "            'POST-GRADUATE','MASTERS',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "def extract_educationDegree(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = []\n",
    "    \n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        text=text.replace(\",\",\" \")\n",
    "        #print(text)\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            #tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            \n",
    "            if tex.upper() in EDUCATIONDEGREE and tex not in STOPWORDS:\n",
    "                \n",
    "                if tex not in edu:\n",
    "                    edu.append(tex)\n",
    "                                \n",
    "                \n",
    "    return edu\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f740b44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.911662Z",
     "iopub.status.busy": "2022-09-16T18:59:07.911264Z",
     "iopub.status.idle": "2022-09-16T18:59:07.922744Z",
     "shell.execute_reply": "2022-09-16T18:59:07.921342Z"
    },
    "papermill": {
     "duration": 0.025161,
     "end_time": "2022-09-16T18:59:07.924916",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.899755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "# Education Degrees\n",
    "Dsignation = [\n",
    "    'assistant professor','associate professor','assistant','affiliate','adjunct faculty',\n",
    "    'associate research professor','associate research scientist','assistant specialist',\n",
    "    'adjunct assistant research scientist',\n",
    "    'instructor',\n",
    "    'manager',\n",
    "    'postdoctoral researcher','program manager','project manager','professor',\n",
    "    'program evaluator','post-doctoral fellow','postdoctoral research fellowship',\n",
    "    'research assistant','research technician',\n",
    "    'senior research associate','staff scientist','seasonal position','specialist',\n",
    "    'teaching assistant','teachers assistant'\n",
    "        ]\n",
    "def extractDsignation(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = []\n",
    "    \n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        text=text.replace(\",\",\" \")\n",
    "        if text.lower() in Dsignation and tex not in STOPWORDS:\n",
    "                \n",
    "            if text not in edu:\n",
    "                edu.append(text)\n",
    "        #print(text)\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            #tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            \n",
    "            if tex.lower() in Dsignation and tex not in STOPWORDS:\n",
    "                \n",
    "                if tex not in edu:\n",
    "                    edu.append(tex)\n",
    "                                \n",
    "                \n",
    "    return edu\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a834e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.947629Z",
     "iopub.status.busy": "2022-09-16T18:59:07.947154Z",
     "iopub.status.idle": "2022-09-16T18:59:07.954809Z",
     "shell.execute_reply": "2022-09-16T18:59:07.953639Z"
    },
    "papermill": {
     "duration": 0.021214,
     "end_time": "2022-09-16T18:59:07.956573",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.935359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def newGetDate(dataMaybeDate):\n",
    "    match = re.search('\\d{4}',dataMaybeDate)\n",
    "    match001=re.search('\\d{2}/\\d{2}/\\d{4}',dataMaybeDate)\n",
    "    \n",
    "    #if match001 is not None and int(match001)>1900 and int(match001)<2023:\n",
    "    if match001:\n",
    "        # Then it found a match!\n",
    "        #print(type(match001))\n",
    "        return match001.group(0)\n",
    "    \n",
    "    if match is not None :\n",
    "        # Then it found a match!\n",
    "        if int(match.group(0))>1900 and int(match.group(0))<2023:\n",
    "            return match.group(0)\n",
    "        return None\n",
    "    return None\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a6c2fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:07.978664Z",
     "iopub.status.busy": "2022-09-16T18:59:07.978321Z",
     "iopub.status.idle": "2022-09-16T18:59:07.985815Z",
     "shell.execute_reply": "2022-09-16T18:59:07.984403Z"
    },
    "papermill": {
     "duration": 0.020677,
     "end_time": "2022-09-16T18:59:07.987835",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.967158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def findInstitute(tika_text):\n",
    "    sub_patterns = ['[A-Z][a-z]* University',\n",
    "                    '[A-Z][a-z]* Educational Institute',\n",
    "                '[A-Z][a-z]* College',\n",
    "                'University of [A-Z][a-z]*',\n",
    "                'The University of [A-Z][a-z]*',\n",
    "                    'TheUniversityof[A-Z][a-z]*',\n",
    "                'Ecole [A-Z][a-z]*',\n",
    "                   '[A-Z][a-z]*University',\n",
    "                    '[A-Z][a-z]*EducationalInstitute',\n",
    "                '[A-Z][a-z]*College',\n",
    "                'Universityof[A-Z][a-z]*',\n",
    "                'Ecole[A-Z][a-z]*',\n",
    "                    'The [A-Z][a-z]* Academy of [A-Z][a-z]*',\n",
    "                    'the [A-Z][a-z]* academy of [A-Z][a-z]*'\n",
    "                   ]\n",
    "    pattern = '({})'.format('|'.join(sub_patterns))\n",
    "    matches = re.findall(pattern, tika_text)\n",
    "\n",
    "    return matches\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cb4517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:08.010969Z",
     "iopub.status.busy": "2022-09-16T18:59:08.010590Z",
     "iopub.status.idle": "2022-09-16T18:59:08.017121Z",
     "shell.execute_reply": "2022-09-16T18:59:08.015871Z"
    },
    "papermill": {
     "duration": 0.020933,
     "end_time": "2022-09-16T18:59:08.019463",
     "exception": false,
     "start_time": "2022-09-16T18:59:07.998530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def removePrefix(dataSent):\n",
    "    resDataIndex=0\n",
    "    for i in dataSent:\n",
    "        #if i.isalpha():\n",
    "        if i.isalnum():\n",
    "            #print(i,resDataIndex)\n",
    "            break\n",
    "        resDataIndex+=1\n",
    "    dataSent=dataSent[resDataIndex:]\n",
    "    return dataSent\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b47c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:08.043180Z",
     "iopub.status.busy": "2022-09-16T18:59:08.042788Z",
     "iopub.status.idle": "2022-09-16T18:59:08.867181Z",
     "shell.execute_reply": "2022-09-16T18:59:08.865944Z"
    },
    "papermill": {
     "duration": 0.838897,
     "end_time": "2022-09-16T18:59:08.869818",
     "exception": false,
     "start_time": "2022-09-16T18:59:08.030921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern], on_match = None)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    res=[]\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        temp=span.text\n",
    "        if len(temp)>0:\n",
    "            lastoption=temp\n",
    "        if (\"vitae\" in temp.lower() or \"sciences\" in temp.lower() \n",
    "            or \"engineering\" in temp.lower() or \"biographical\" in temp.lower() \n",
    "            or \"no\" in temp.lower() or \"title\" in temp.lower() ):\n",
    "            continue\n",
    "        res.append(temp)\n",
    "    \n",
    "    return res\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0608fd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:08.894832Z",
     "iopub.status.busy": "2022-09-16T18:59:08.894439Z",
     "iopub.status.idle": "2022-09-16T18:59:08.899891Z",
     "shell.execute_reply": "2022-09-16T18:59:08.898877Z"
    },
    "papermill": {
     "duration": 0.021967,
     "end_time": "2022-09-16T18:59:08.903198",
     "exception": false,
     "start_time": "2022-09-16T18:59:08.881231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#E-MAIL\n",
    "#import re\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7682cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:08.926867Z",
     "iopub.status.busy": "2022-09-16T18:59:08.926456Z",
     "iopub.status.idle": "2022-09-16T18:59:08.940884Z",
     "shell.execute_reply": "2022-09-16T18:59:08.939813Z"
    },
    "papermill": {
     "duration": 0.02925,
     "end_time": "2022-09-16T18:59:08.943479",
     "exception": false,
     "start_time": "2022-09-16T18:59:08.914229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleEducations(listOfEducations):\n",
    "    eduData=[]\n",
    "    pieceEduData=dict()\n",
    "    rest=\"\"\n",
    "    for eduPart in listOfEducations:\n",
    "        #print(type(eduPart))\n",
    "        \n",
    "        res=extract_educationDegree(eduPart)\n",
    "        if res:\n",
    "            if len(pieceEduData)!=0:\n",
    "                if len(rest)>1:\n",
    "                    pieceEduData[\"total details\"]=rest\n",
    "                    rest=\"\"\n",
    "                eduData.append(pieceEduData)\n",
    "                pieceEduData=dict()\n",
    "            pieceEduData[\"degree\"]=res[0]\n",
    "            index001=eduPart.find(res[0])\n",
    "            eduPart=eduPart[:index001]+eduPart[index001+len(res[0]):]\n",
    "            #print(res,index001,eduPart)\n",
    "            #print(res,eduPart)\n",
    "        #datePart=getDate(eduPart)\n",
    "        newDatePart=newGetDate(eduPart)\n",
    "        \n",
    "        #if datePart:\n",
    "        #    print(\"date:\",datePart)\n",
    "        if newDatePart:\n",
    "            pieceEduData[\"date\"]=newDatePart\n",
    "            index002=eduPart.find(newDatePart)\n",
    "            eduPart=eduPart[:index002]+eduPart[index002+len(newDatePart):]\n",
    "            #print(index002,\"date:\",newDatePart,eduPart)\n",
    "        \n",
    "        #test001=eduPart.split(\":\")\n",
    "        #for itest in test001:\n",
    "        if \"advisor\" in eduPart.lower():\n",
    "            \n",
    "            index003=eduPart.lower().find(\"advisor\")\n",
    "            advis001=eduPart[index003+len(\"advisor\"):]\n",
    "            advis001=removePrefix(advis001)\n",
    "            pieceEduData[\"advisor\"]=advis001\n",
    "            eduPart=eduPart[:index003]\n",
    "            #print(\"advisor\",advis001,eduPart)\n",
    "        org=findInstitute(eduPart)\n",
    "        if org:\n",
    "            pieceEduData[\"organization\"]=org[0]\n",
    "            index004=eduPart.find(org[0])\n",
    "            eduPart=eduPart[:index004]+eduPart[index004+len(org[0]):]\n",
    "            #print(org,eduPart)\n",
    "        #print(eduPart)\n",
    "        eduPart=removePrefix(eduPart)\n",
    "        \n",
    "        designation=extractDsignation(eduPart)\n",
    "        if designation:\n",
    "            pieceEduData[\"designation\"]=designation[0]\n",
    "            index004=eduPart.find(designation[0])\n",
    "            eduPart=eduPart[:index004]+eduPart[index004+len(designation[0]):]\n",
    "        \n",
    "        if len(eduPart)>1:\n",
    "            rest+=eduPart\n",
    "        #print(eduPart)\n",
    "        \n",
    "        \n",
    "    if len(pieceEduData)!=0:\n",
    "        if len(rest)>1:\n",
    "            pieceEduData[\"total details\"]=rest\n",
    "                    #rest=\"\"\n",
    "        eduData.append(pieceEduData)\n",
    "    return eduData\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f7a4f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:08.967479Z",
     "iopub.status.busy": "2022-09-16T18:59:08.967062Z",
     "iopub.status.idle": "2022-09-16T18:59:08.976075Z",
     "shell.execute_reply": "2022-09-16T18:59:08.975000Z"
    },
    "papermill": {
     "duration": 0.02432,
     "end_time": "2022-09-16T18:59:08.979004",
     "exception": false,
     "start_time": "2022-09-16T18:59:08.954684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleContacts(sentDataContacts):\n",
    "    \n",
    "    contactDetails=dict()\n",
    "    \n",
    "    #otherDetails=[]\n",
    "    \n",
    "    #print(sentDataContacts)\n",
    "    textContacts001=\",\".join(sentDataContacts)\n",
    "    nameContacts001=extract_name(textContacts001)\n",
    "    if nameContacts001:\n",
    "        contactDetails[\"name\"]=nameContacts001[0]\n",
    "    else:\n",
    "        contactDetails[\"name\"]=sentDataContacts[0]\n",
    "    \n",
    "    #flagContact=False\n",
    "    othersRests=[]\n",
    "    #print(nameContacts001[0],nameContacts001)\n",
    "    for contacts00 in sentDataContacts:\n",
    "        #dateFound001=newGetDate(contacts00)\n",
    "        #print(dateFound001,contacts00)\n",
    "        \n",
    "        \n",
    "        \n",
    "        e_mailContacts=get_email_addresses(contacts00)\n",
    "        if e_mailContacts:\n",
    "            #print(e_mailContacts,contacts00)\n",
    "            #contactIndex001=contacts00.lower().find(e_mailContacts)\n",
    "            contactIndex001=contacts00.find(e_mailContacts[0])\n",
    "            \n",
    "            contacts00=contacts00[:contactIndex001]+contacts00[contactIndex001+len(e_mailContacts[0]):]\n",
    "            contacts00=removePrefix(contacts00)\n",
    "            \n",
    "            \n",
    "            contactDetails[\"e-mails\"]=e_mailContacts[0]\n",
    "            #print(e_mailContacts,contacts00)\n",
    "        othersRests.append(contacts00)\n",
    "    contactDetails[\"other_details\"]=\" \".join(othersRests)\n",
    "    return contactDetails\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f177f71d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:09.004206Z",
     "iopub.status.busy": "2022-09-16T18:59:09.003815Z",
     "iopub.status.idle": "2022-09-16T18:59:09.026014Z",
     "shell.execute_reply": "2022-09-16T18:59:09.025210Z"
    },
    "papermill": {
     "duration": 0.037798,
     "end_time": "2022-09-16T18:59:09.028405",
     "exception": false,
     "start_time": "2022-09-16T18:59:08.990607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "def handleRestPart(experiencePart):\n",
    "    allExperience=[]\n",
    "    detailsExperience=\"\"\n",
    "    tempExperience=dict()\n",
    "    flag=False # need for different in the 1st\n",
    "    for partExperience in experiencePart:\n",
    "        dateFound=newGetDate(partExperience)\n",
    "        if dateFound:\n",
    "            \n",
    "            \n",
    "            if not flag:\n",
    "                flag=True\n",
    "                tempExperience['year']=dateFound\n",
    "                partExperienceIndex001=partExperience.find(dateFound)\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dateFound):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                detailsExperience+=\" \"+partExperience\n",
    "                \n",
    "                organizationExperience=findInstitute(partExperience)\n",
    "        \n",
    "                if organizationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(organizationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['organization']=organizationExperience[0]\n",
    "                \n",
    "                dsignationExperience=extractDsignation(partExperience)\n",
    "                if dsignationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['dsignation']=dsignationExperience[0]\n",
    "                \n",
    "                continue\n",
    "            else:\n",
    "                tempExperience['details']=detailsExperience\n",
    "                detailsExperience=\"\"\n",
    "                allExperience.append(tempExperience)\n",
    "                \n",
    "                tempExperience=dict()\n",
    "                tempExperience['year']=dateFound\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(dateFound)\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dateFound):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                organizationPublications=findInstitute(partExperience)\n",
    "        \n",
    "                if organizationPublications:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(organizationPublications[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationPublications[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['organization']=organizationPublications[0]\n",
    "                dsignationExperience=extractDsignation(partExperience)\n",
    "                if dsignationExperience:\n",
    "                \n",
    "                    partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                    partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                    partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                    tempExperience['dsignation']=dsignationExperience[0]\n",
    "                \n",
    "                detailsExperience+=\" \"+partExperience\n",
    "        else:\n",
    "            partExperience=removePrefix(partExperience)\n",
    "            \n",
    "            organizationPublications=findInstitute(partExperience)\n",
    "        \n",
    "            if organizationPublications:\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(organizationPublications[0])\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(organizationPublications[0]):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                tempExperience['organization']=organizationPublications[0]\n",
    "            \n",
    "            \n",
    "            dsignationExperience=extractDsignation(partExperience)\n",
    "            if dsignationExperience:\n",
    "                \n",
    "                partExperienceIndex001=partExperience.find(dsignationExperience[0])\n",
    "            \n",
    "                partExperience=partExperience[:partExperienceIndex001]+partExperience[partExperienceIndex001+len(dsignationExperience[0]):]\n",
    "                partExperience=removePrefix(partExperience)\n",
    "                \n",
    "                tempExperience['dsignation']=dsignationExperience[0]\n",
    "            \n",
    "            detailsExperience+=\" \"+partExperience\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # extracting name gives a lot of false result, \n",
    "        #as it contains name of things other than person name\n",
    "        \n",
    "        #namepartExperience=extract_name(partExperience)\n",
    "        #if namepartExperience:\n",
    "            #print(namepartExperience,type(namepartExperience))\n",
    "    \n",
    "        \n",
    "        \n",
    "    if tempExperience:\n",
    "        tempExperience['details']=detailsExperience\n",
    "        allExperience.append(tempExperience)\n",
    "            \n",
    "    return allExperience\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd733c",
   "metadata": {
    "papermill": {
     "duration": 0.010892,
     "end_time": "2022-09-16T18:59:09.050890",
     "exception": false,
     "start_time": "2022-09-16T18:59:09.039998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84fd1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:09.075986Z",
     "iopub.status.busy": "2022-09-16T18:59:09.074805Z",
     "iopub.status.idle": "2022-09-16T18:59:10.476742Z",
     "shell.execute_reply": "2022-09-16T18:59:10.475444Z"
    },
    "papermill": {
     "duration": 1.417316,
     "end_time": "2022-09-16T18:59:10.479389",
     "exception": false,
     "start_time": "2022-09-16T18:59:09.062073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "#import pdfplumber\n",
    "#import pandas as pd\n",
    "#import os\n",
    "# Importing required modules\n",
    "import PyPDF2\n",
    "\n",
    "def extract_pdf(pdf_path):\n",
    "    linesOfFile = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for pdf_page in pdf.pages:\n",
    "            single_page_text = pdf_page.extract_text()\n",
    "            for line in single_page_text.split('\\n'):\n",
    "                linesOfFile.append(line)\n",
    "                #print(linesOfFile)\n",
    "    return linesOfFile\n",
    "\n",
    "\n",
    "folder_with_pdfs = '../input/cvalltogether'\n",
    "linesOfFiles = []\n",
    "\n",
    "#specially for testing ner\n",
    "dataForTest=dict()\n",
    "\n",
    "listOfPdfFiles=[]\n",
    "\n",
    "for pdf_file in os.listdir(folder_with_pdfs):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        \n",
    "        #print(pdf_file)\n",
    "        listOfPdfFiles.append(pdf_file)\n",
    "        pdf_file_path = os.path.join(folder_with_pdfs, pdf_file)\n",
    "        \n",
    "        pdfFileObj = open(pdf_file_path,'rb')\n",
    "        \n",
    "        #print(pdfFileObj)\n",
    "        \n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# Getting number of pages in pdf file\n",
    "        pages = pdfReader.numPages\n",
    "\n",
    "        totalText=[]\n",
    "\n",
    "# Loop for reading all the Pages\n",
    "        for i in range(pages):\n",
    "\n",
    "        # Creating a page object\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "\n",
    "        # Printing Page Number\n",
    "        #print(\"Page No: \",i)\n",
    "\n",
    "        # Extracting text from page\n",
    "        # And splitting it into chunks of lines\n",
    "            text = pageObj.extractText().split(\"\\n\")\n",
    "        \n",
    "            totalText+=text\n",
    "        \n",
    "        \n",
    "# closing the pdf file object\n",
    "        pdfFileObj.close()\n",
    "        #print(totalText)\n",
    "        \n",
    "        #sectionDictionary=dfPartByPart(totalText)\n",
    "        #newSectionDictionary=reDistribute(sectionDictionary)\n",
    "        \n",
    "        \n",
    "        newSectionDictionary=dfPartByPart(totalText)\n",
    "        \n",
    "        #specially for testing ner\n",
    "        dataForTest=newSectionDictionary\n",
    "        \n",
    "        tempData00=[]\n",
    "        \n",
    "        contactPart001,educationsPart001=dict(),dict()\n",
    "        \n",
    "        personName00=\"\"\n",
    "        personNameLast=\"\"\n",
    "        if 'contacts' in newSectionDictionary:\n",
    "            text00=newSectionDictionary['contacts']\n",
    "            personName00=extract_name(\",\".join(text00))\n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        \n",
    "        if len(personName00)==0:\n",
    "            if 'introductions' in newSectionDictionary:\n",
    "                text00=newSectionDictionary['introductions']\n",
    "                personName00=extract_name(\",\".join(text00))\n",
    "            \n",
    "                \n",
    "            if len(personName00)==0:\n",
    "                personNameLast=\"no name\"\n",
    "            else:\n",
    "             \n",
    "            \n",
    "                personNameLast=personName00[0]\n",
    "        else:\n",
    "             \n",
    "            \n",
    "            personNameLast=personName00[0]\n",
    "        #print(i,personNameLast)\n",
    "\n",
    "        \n",
    "        \n",
    "        for partSections in newSectionDictionary:\n",
    "            tempData00=[personNameLast]\n",
    "            if (partSections=='contacts' or partSections=='introductions') and newSectionDictionary[partSections]:\n",
    "                tempData00.append(partSections+\" details\")\n",
    "                tempData00.append(partSections)\n",
    "                contactPart001=handleContacts(newSectionDictionary[partSections])\n",
    "                for parts in contactPart001:\n",
    "                    tempData00.append(parts)\n",
    "                    tempData00.append(contactPart001[parts])\n",
    "                \n",
    "                #print(tempData00)\n",
    "                    if len(tempData00)>1:\n",
    "                        allCVData.append(tempData00)\n",
    "                \n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "            \n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "            \n",
    "            #print(contactPart001) \n",
    "            #done for now\n",
    "            elif (partSections=='educations' or partSections=='certificates' or partSections=='certifications') and newSectionDictionary[partSections]:\n",
    "                tempData00.append(\"qualifications details\")\n",
    "                tempData00.append(partSections)\n",
    "                educationsPart001=handleEducations(newSectionDictionary[partSections])\n",
    "                for parts001 in educationsPart001:\n",
    "                    tempData00.append(\"degree earned\")\n",
    "                    if 'degree' in parts001:\n",
    "                        tempData00.append(parts001['degree'])\n",
    "                    else:\n",
    "                        tempData00.append(\"Degree\")\n",
    "                \n",
    "                    for smallparts002 in parts001:\n",
    "                        tempData00.append(smallparts002)\n",
    "                        tempData00.append(parts001[smallparts002])\n",
    "                    \n",
    "                    #print(tempData00)\n",
    "                        if len(tempData00)>1:\n",
    "                            allCVData.append(tempData00.copy())\n",
    "                    \n",
    "                        tempData00.pop()\n",
    "                        tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "            else:\n",
    "                tempEdge=partSections +\" experience\"\n",
    "        \n",
    "                tempData00.append(tempEdge)\n",
    "                tempData00.append(partSections)\n",
    "                restPart001=handleRestPart(newSectionDictionary[partSections])\n",
    "        \n",
    "                count=1\n",
    "                for parts001 in restPart001:\n",
    "                    newTempEdge=partSections +\" details\"\n",
    "                    tempData00.append(newTempEdge)\n",
    "                    tempData00.append(partSections+\" no.\"+str(count))\n",
    "            \n",
    "            \n",
    "                \n",
    "                    for smallparts002 in parts001:\n",
    "                        tempData00.append(smallparts002)\n",
    "                        tempData00.append(parts001[smallparts002])\n",
    "                    \n",
    "                    #print(tempData00)\n",
    "                        if len(tempData00)>1:\n",
    "                            allCVData.append(tempData00.copy())\n",
    "                    \n",
    "                        tempData00.pop()\n",
    "                        tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    tempData00.pop()\n",
    "                    count+=1\n",
    "                tempData00.pop()\n",
    "                tempData00.pop()\n",
    "#print(\"complete\")\n",
    "        #break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "        \n",
    "#df = pd.DataFrame(linesOfFiles)\n",
    "#df.to_csv('test.csv')\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0bd7b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:10.503931Z",
     "iopub.status.busy": "2022-09-16T18:59:10.503554Z",
     "iopub.status.idle": "2022-09-16T18:59:10.509613Z",
     "shell.execute_reply": "2022-09-16T18:59:10.508488Z"
    },
    "papermill": {
     "duration": 0.021384,
     "end_time": "2022-09-16T18:59:10.512140",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.490756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'year', '1995'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'organization', 'York University'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.1', 'details', ' Education Ph.D. Courant Institute, New York University, New York, NY.'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'year', '1990'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'organization', 'Stanford University'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.2', 'details', ' B.S. , Stanford, CA.'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.3', 'year', '1986'], ['Sunder Sethuraman', 'education experience', 'education', 'education details', 'education no.3', 'details', ' Diploma Florida High School, Tallahassee, FL.'], ['Sunder Sethuraman', 'professional experience', 'professional', 'professional details', 'professional no.1', 'year', '2011'], ['Sunder Sethuraman', 'professional experience', 'professional', 'professional details', 'professional no.1', 'dsignation', 'Professor']]\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "totalAllCVData=[]\n",
    "for check in allCVData:\n",
    "    if len(check)>1:\n",
    "        totalAllCVData.append(check)\n",
    "print(totalAllCVData[:10])\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cddffe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:10.538037Z",
     "iopub.status.busy": "2022-09-16T18:59:10.537109Z",
     "iopub.status.idle": "2022-09-16T18:59:10.544247Z",
     "shell.execute_reply": "2022-09-16T18:59:10.542923Z"
    },
    "papermill": {
     "duration": 0.021811,
     "end_time": "2022-09-16T18:59:10.546426",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.524615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#write.csv(allCVData, \"mycsv.csv\")\n",
    "\n",
    "#header = ['head','relation','tail']\n",
    "\n",
    "with open('CVpdftotalAllCVData001.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header , already has a header\n",
    "    #writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(totalAllCVData)\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f06db9",
   "metadata": {
    "papermill": {
     "duration": 0.011223,
     "end_time": "2022-09-16T18:59:10.569770",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.558547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f7ad6",
   "metadata": {
    "papermill": {
     "duration": 0.011182,
     "end_time": "2022-09-16T18:59:10.592136",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.580954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061de5f",
   "metadata": {
    "papermill": {
     "duration": 0.010977,
     "end_time": "2022-09-16T18:59:10.614393",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.603416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c647ee",
   "metadata": {
    "papermill": {
     "duration": 0.010913,
     "end_time": "2022-09-16T18:59:10.636694",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.625781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a3633",
   "metadata": {
    "papermill": {
     "duration": 0.011088,
     "end_time": "2022-09-16T18:59:10.659182",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.648094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e16dc1",
   "metadata": {
    "papermill": {
     "duration": 0.011229,
     "end_time": "2022-09-16T18:59:10.682037",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.670808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea437c3",
   "metadata": {
    "papermill": {
     "duration": 0.011366,
     "end_time": "2022-09-16T18:59:10.705086",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.693720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdf04a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:10.730387Z",
     "iopub.status.busy": "2022-09-16T18:59:10.729346Z",
     "iopub.status.idle": "2022-09-16T18:59:11.404900Z",
     "shell.execute_reply": "2022-09-16T18:59:11.403450Z"
    },
    "papermill": {
     "duration": 0.690886,
     "end_time": "2022-09-16T18:59:11.407617",
     "exception": false,
     "start_time": "2022-09-16T18:59:10.716731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en')\n",
    "def nerCV(personName,sectionName,sentDataFoeTest):\n",
    "    allDataner=[]\n",
    "    metadata001=[personName,sectionName+\" with\",sectionName]\n",
    "    countNo=0\n",
    "    for train_dataText in sentDataFoeTest :\n",
    "        \n",
    "    \n",
    "    #train_dataText=\" \".join(sentDataFoeTest)\n",
    "    #doc = loaded_nlp_model(train_dataText)\n",
    "    \n",
    "        doc = nlp(train_dataText)\n",
    "        #print(doc)\n",
    "        \n",
    "        dldata=dict()\n",
    "    #print(doc)\n",
    "        #print(\"*************\")\n",
    "        for ent in doc.ents:\n",
    "        #print(\"as\")\n",
    "            if ent.label_.upper() in dldata:\n",
    "                dldata[ent.label_.upper()].append(ent.text)\n",
    "            else:\n",
    "                dldata[ent.label_.upper()]=[ent.text]\n",
    "            #print(ent.label_.upper())\n",
    "            #print(f'{ent.label_.upper():{30}}- {ent.text}')\n",
    "    #break\n",
    "        if dldata:\n",
    "            countNo+=1\n",
    "            #print(dldata)\n",
    "            #$allDataner.append(dldata)\n",
    "            for dl in dldata:\n",
    "                metadata=metadata001.copy()\n",
    "                if dl=='DATE':\n",
    "                    for d in dldata[dl]:\n",
    "                        if len(d)<3:\n",
    "                            continue\n",
    "                        metadata.append(sectionName+ \" in \"+str(dl)+\" \"+str(countNo)+\" \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        #print(metadata)\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='ORG':\n",
    "                    for d in dldata[dl]:\n",
    "                        if len(d)<3:\n",
    "                            continue\n",
    "                        metadata.append(sectionName+ \" with \"+str(dl)+\" \"+str(countNo)+\" \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='PERSON':\n",
    "                    for d in dldata[dl]:\n",
    "                        if len(d)<3:\n",
    "                            continue\n",
    "                        metadata.append(sectionName+ \" associated with \"+str(dl)+\" \"+str(countNo)+\" \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                elif dl=='GPE':\n",
    "                    for d in dldata[dl]:\n",
    "                        if len(d)<3:\n",
    "                            continue\n",
    "                        metadata.append(sectionName+ \" from \"+str(dl)+\" \"+str(countNo)+\" \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "                else:\n",
    "                    for d in dldata[dl]:\n",
    "                        if len(d)<3:\n",
    "                            continue\n",
    "                        metadata.append(sectionName+ \" in \"+str(dl)+\" \"+str(countNo)+\" \")\n",
    "                        metadata.append(d)\n",
    "                        allDataner.append(metadata.copy())\n",
    "                        metadata.pop()\n",
    "                        metadata.pop()\n",
    "            #\n",
    "            metadata.append(\"Original text \"+str(countNo)+\" \")\n",
    "            metadata.append(train_dataText)\n",
    "            allDataner.append(metadata.copy())\n",
    "            metadata.pop()\n",
    "            metadata.pop()\n",
    "    #print(allDataner)\n",
    "    return allDataner\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0790056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T18:59:11.433580Z",
     "iopub.status.busy": "2022-09-16T18:59:11.432736Z",
     "iopub.status.idle": "2022-09-16T19:00:56.461126Z",
     "shell.execute_reply": "2022-09-16T19:00:56.459885Z"
    },
    "papermill": {
     "duration": 105.055726,
     "end_time": "2022-09-16T19:00:56.475377",
     "exception": false,
     "start_time": "2022-09-16T18:59:11.419651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "folder_with_pdfs = '../input/cvalltogether'\n",
    "linesOfFiles = []\n",
    "\n",
    "#specially for testing ner\n",
    "dataForTest=dict()\n",
    "\n",
    "listOfPdfFiles=[]\n",
    "\n",
    "alnerCVData=[]\n",
    "\n",
    "for pdf_file in os.listdir(folder_with_pdfs):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        \n",
    "        #print(pdf_file)\n",
    "        listOfPdfFiles.append(pdf_file)\n",
    "        pdf_file_path = os.path.join(folder_with_pdfs, pdf_file)\n",
    "        \n",
    "        pdfFileObj = open(pdf_file_path,'rb')\n",
    "        \n",
    "        #print(pdfFileObj)\n",
    "        \n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# Getting number of pages in pdf file\n",
    "        pages = pdfReader.numPages\n",
    "\n",
    "        totalText=[]\n",
    "\n",
    "# Loop for reading all the Pages\n",
    "        for i in range(pages):\n",
    "\n",
    "        # Creating a page object\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "\n",
    "        # Printing Page Number\n",
    "        #print(\"Page No: \",i)\n",
    "\n",
    "        # Extracting text from page\n",
    "        # And splitting it into chunks of lines\n",
    "            text = pageObj.extractText().split(\"\\n\")\n",
    "        \n",
    "            totalText+=text\n",
    "        \n",
    "        \n",
    "# closing the pdf file object\n",
    "        pdfFileObj.close()\n",
    "        #print(totalText)\n",
    "        \n",
    "        #sectionDictionary=dfPartByPart(totalText)\n",
    "        #newSectionDictionary=reDistribute(sectionDictionary)\n",
    "        \n",
    "        \n",
    "        newSectionDictionary=dfPartByPart(totalText)\n",
    "        \n",
    "        personName00=\"\"\n",
    "        personNameLast=\"\"\n",
    "        if 'contacts' in newSectionDictionary:\n",
    "            text00=newSectionDictionary['contacts']\n",
    "            personName00=extract_name(\",\".join(text00))\n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        \n",
    "        if len(personName00)==0:\n",
    "            if 'introductions' in newSectionDictionary:\n",
    "                text00=newSectionDictionary['introductions']\n",
    "                personName00=extract_name(\",\".join(text00))\n",
    "            \n",
    "                \n",
    "            if len(personName00)==0:\n",
    "                personNameLast=\"no name\"\n",
    "            else:\n",
    "             \n",
    "            \n",
    "                personNameLast=personName00[0]\n",
    "        else:\n",
    "             \n",
    "            \n",
    "            personNameLast=personName00[0]\n",
    "        \n",
    "        #specially for testing ner\n",
    "        dataForTest=newSectionDictionary\n",
    "        for sectionDataSentTest in dataForTest:\n",
    "            if len(dataForTest[sectionDataSentTest])>1:\n",
    "                nerret=nerCV(personNameLast,sectionDataSentTest,dataForTest[sectionDataSentTest][1:])\n",
    "                #print(nerret)\n",
    "                #alnerCVData.extend()\n",
    "                for i in nerret:\n",
    "                    if i not in alnerCVData:\n",
    "                        alnerCVData.append(i)\n",
    "                \n",
    "#dataForTest\n",
    "        #break\n",
    "#alnerCVData=list(set(alnerCVData))\n",
    "#print(alnerCVData)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a092b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:00:56.502697Z",
     "iopub.status.busy": "2022-09-16T19:00:56.501552Z",
     "iopub.status.idle": "2022-09-16T19:00:56.507737Z",
     "shell.execute_reply": "2022-09-16T19:00:56.506607Z"
    },
    "papermill": {
     "duration": 0.02226,
     "end_time": "2022-09-16T19:00:56.510071",
     "exception": false,
     "start_time": "2022-09-16T19:00:56.487811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunder Sethuraman', 'introductions with', 'introductions', 'introductions with ORG 1 ', 'WWW'], ['Sunder Sethuraman', 'introductions with', 'introductions', 'Original text 1 ', 'WWW: http://www.math.arizona.edu/~ sethuram/'], ['Sunder Sethuraman', 'education with', 'education', 'education in DATE 1 ', '1995'], ['Sunder Sethuraman', 'education with', 'education', 'education with ORG 1 ', 'Ph.D. Courant Institute'], ['Sunder Sethuraman', 'education with', 'education', 'education with ORG 1 ', 'New York University'], ['Sunder Sethuraman', 'education with', 'education', 'education from GPE 1 ', 'New York'], ['Sunder Sethuraman', 'education with', 'education', 'Original text 1 ', '1995 Ph.D. Courant Institute, New York University, New York, NY.'], ['Sunder Sethuraman', 'education with', 'education', 'education in DATE 2 ', '1990'], ['Sunder Sethuraman', 'education with', 'education', 'education with ORG 2 ', 'B.S. Stanford University'], ['Sunder Sethuraman', 'education with', 'education', 'education with ORG 2 ', 'Stanford']]\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "print(alnerCVData[:10])\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e57057",
   "metadata": {
    "papermill": {
     "duration": 0.011704,
     "end_time": "2022-09-16T19:00:56.534040",
     "exception": false,
     "start_time": "2022-09-16T19:00:56.522336",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "230b997b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:00:56.559918Z",
     "iopub.status.busy": "2022-09-16T19:00:56.559142Z",
     "iopub.status.idle": "2022-09-16T19:00:57.467878Z",
     "shell.execute_reply": "2022-09-16T19:00:57.466709Z"
    },
    "papermill": {
     "duration": 0.924569,
     "end_time": "2022-09-16T19:00:57.470411",
     "exception": false,
     "start_time": "2022-09-16T19:00:56.545842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sectionDataSentTest in dataForTest:\n",
    "    if len(dataForTest[sectionDataSentTest])>1:\n",
    "        nerret=nerCV(personNameLast,sectionDataSentTest,dataForTest[sectionDataSentTest][1:])\n",
    "        \n",
    "        #print(nerret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0df6f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T19:00:57.495937Z",
     "iopub.status.busy": "2022-09-16T19:00:57.495134Z",
     "iopub.status.idle": "2022-09-16T19:00:57.561122Z",
     "shell.execute_reply": "2022-09-16T19:00:57.559907Z"
    },
    "papermill": {
     "duration": 0.081327,
     "end_time": "2022-09-16T19:00:57.563637",
     "exception": false,
     "start_time": "2022-09-16T19:00:57.482310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "with open('nerCVpdftotalAllCVData001.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header , already has a header\n",
    "    #writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(alnerCVData)\n",
    "\n",
    "\n",
    "print(\"complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 160.179753,
   "end_time": "2022-09-16T19:01:00.905779",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-16T18:58:20.726026",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
